{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMpdeh26FoW6"
      },
      "source": [
        "# **AI-Driven NLP: Language Model Exploration**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE6EzX22HeqS"
      },
      "source": [
        " ## **Import Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puwoYaPeFmPm"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import google.generativeai as genai\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t_HA-bQH1kH"
      },
      "source": [
        "## **Load Pre-trained Language Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVkzcXxEFmSf"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_model(model_name=\"gpt2\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    return tokenizer, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbECpg7VFmWS"
      },
      "outputs": [],
      "source": [
        "tokenizer, model = load_model(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hs7nf5pIBMu"
      },
      "source": [
        "## **Define Inference Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWi8SPt8FmZs"
      },
      "outputs": [],
      "source": [
        "def generate_text(prompt, max_length=100):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs, max_length=max_length)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9MBJDF_IQCS"
      },
      "source": [
        "## **Test Model Output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YImTb2_4ILfh"
      },
      "outputs": [],
      "source": [
        "prompt = \"Once upon a time, in a distant land\"\n",
        "generated_text = generate_text(prompt)\n",
        "print(\"Generated Text:\\n\", generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMS70zxeIctj"
      },
      "outputs": [],
      "source": [
        "def setup_gemini(api_key, model_name=\"gemini-1.5-pro-latest\"):\n",
        "    genai.configure(api_key=api_key)\n",
        "    model = genai.GenerativeModel(model_name)\n",
        "    return model\n",
        "\n",
        "def generate_gemini_text(model, prompt):\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text\n",
        "\n",
        "api_key = \"AIzaSyPaVjwVTcEGd9uSgTVf-FigBSvrsQL987Y\"\n",
        "gemini_model = setup_gemini(api_key, \"gemini-1.5-pro-latest\")\n",
        "gemini_response = generate_gemini_text(gemini_model, prompt)\n",
        "print(\"Gemini Output:\\n\", gemini_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PolXu_4jIzeR"
      },
      "source": [
        "## **Research Questions & Objectives**\n",
        "* How well does GPT-2 understand contextual prompts?\n",
        "* Can GPT-2 generate coherent and creative responses?\n",
        "* What are the limitations of GPT-2 in handling ambiguous inputs?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig_qB_oXJDfJ"
      },
      "source": [
        "## **Implementation of GPT-2**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dr5opzOvJKt2"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Load GPT-2 model and tokenizer\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIJkXca-JXzx"
      },
      "source": [
        "## **Exploration & Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5tU0vB9JRcQ"
      },
      "outputs": [],
      "source": [
        "sample_prompts = [\n",
        "    \"Once upon a time in a futuristic city, there was a robot who\",\n",
        "    \"The key difference between classical physics and quantum mechanics is\",\n",
        "    \"A fascinating fact about black holes is that\"\n",
        "]\n",
        "\n",
        "for prompt in sample_prompts:\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    output = model.generate(**inputs, max_length=50)\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Print only the prompt and generated text without unnecessary messages\n",
        "    print(f'Prompt: {prompt}\\nGenerated: {generated_text}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mQMMSMHJgxA"
      },
      "source": [
        "## **Visualization of Results**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxUDxd2_JqVV"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "reference = \"Once upon a time in a futuristic city, there was a robot who helped humans.\"\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "bleu_score = sentence_bleu([reference.split()], generated_text.split())\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = scorer.score(reference, generated_text)\n",
        "\n",
        "print(f'BLEU Score: {bleu_score}')\n",
        "print(f'ROUGE Scores: {rouge_scores}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5dtWwguJwW5"
      },
      "source": [
        "## **Conclusion & Insights**\n",
        "* While GPT-2 is capable of generating fluent and grammatically correct text, it may face challenges with maintaining coherence over longer passages.\n",
        "* Its ability to understand context is heavily influenced by how prompts are structured.\n",
        "* To enhance its performance, especially in domain-specific tasks, further fine-tuning on targeted datasets is recommended."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
